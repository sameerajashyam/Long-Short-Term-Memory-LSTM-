
import pandas as pd
import  numpy as np
import nltk
nltk.download('gutenberg')
print('---------------------------')

from nltk.corpus import gutenberg

data=gutenberg.raw('shakespeare-hamlet.txt')
with open('hamlet.txt','w') as file:
    file.write(data)

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

with open('hamlet.txt','r') as file:
    text=file.read().lower()


tokenizer=Tokenizer()
tokenizer.fit_on_texts([text])
total_words=len(tokenizer.word_index)+1
print(total_words)
print(tokenizer.word_index)

print('-----------------------------')
input_sequences=[]
for line in text.split('\n'):
    token_list=tokenizer.texts_to_sequences([line])
    for i in range(1,len(token_list)):
        n_gram_sequence=token_list[:i+1]
        input_sequences.append((n_gram_sequence))
print(input_sequences)

max_sequence_len=max([len(x) for x in input_sequences])
print(max_sequence_len)

input_sequences=np.array(pad_sequences(input_sequences,maxlen=max_sequence_len,padding='pre'))
print(input_sequences)

##create predicitors and label
import tensorflow as tf
x,y=input_sequences[:,:-1],input_sequences[:,-1]


y=tf.keras.utils.to_categorical(y,num_classes=total_words)
print(y)

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

# Define early stopping
from tensorflow.keras.callbacks import EarlyStopping
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

print('---------------------------')


